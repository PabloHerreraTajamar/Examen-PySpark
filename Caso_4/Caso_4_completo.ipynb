{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6faa8f82-8459-4b9d-b8be-b985c9f761af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Limpieza de datos con PySpark: Data Science Job Posting on Glassdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffad86de-2091-43f9-97e4-c94456fee761",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Los [datos](https://tajamar365.sharepoint.com/:x:/s/3405-MasterIA2024-2025/ETYTQ0c-i6FLjM8rZ4iT1cgB6ipFAkainM-4V9M8DXsBiA?e=PeMtvh) fueron extraídos (scrapeados) del sitio web de Glassdoor y recoge los salarios de distintos puestos relacionados a Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbd64de3-16db-4ce1-89ee-026c842262a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Resolver los siguientes requerimientos, para cada operación/moficación imprima como van quedadndo los cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55506577-cd5d-44fc-9b5f-eb6f055e3e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "filepath = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22e55e35-5fe8-4a9a-a14a-d8d15dbe45b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. Cargar los datos y mostrar el esquema o la informacion de las columnas y el tip de dato de cada columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69b4dff8-60d1-401a-bb1a-bb3b638fb2f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import (\n",
    "    SparkSession,\n",
    "    types,\n",
    "    functions as F\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder \n",
    "    .appName(\"Job posting\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfe4f3d-e113-4d7c-b186-24e02d52d258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \";\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"/FileStore/Examen/Caso_4/ds_jobs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63a94d0f-419b-4379-83e1-2aa880f171fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7f47e8-e3a7-49f9-ac5a-06a272fe79b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a4b636b-5e44-4afa-9d44-0666b991cec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. Eliminar duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ba7a3d-4424-47d8-9760-a74922d3ffde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar las funciones necesarias\n",
    "from pyspark.sql.functions import col, count\n",
    "\n",
    "# Agrupar por todas las columnas y contar las ocurrencias\n",
    "df_duplicates = df.groupBy(df.columns).agg(count(\"*\").alias(\"count\"))\n",
    "\n",
    "# Filtrar solo los duplicados\n",
    "df_duplicates = df_duplicates.filter(col(\"count\") > 1)\n",
    "\n",
    "# Mostrar los duplicados\n",
    "df_duplicates.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89bca61f-7eb0-4082-838d-de4d27accfc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar los registros duplicados basados en todas las columnas, aunque en este caso no se encuentra ninguno\n",
    "df_clean = df.dropDuplicates()\n",
    "\n",
    "# Mostrar el DataFrame limpio\n",
    "df_clean.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9b97746-8b29-4550-9e1b-be2876c2a0a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Decidir que hacer con los datos faltantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5dfc8f7-a71f-4fe4-9c45-146a6af535d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaedda51-4a41-41cd-aba7-75f0e5a3c5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. Decidir que hacer con los valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f043400-94c8-4ac7-9693-c54bde101a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Eliminar los datos nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "094c9827-e829-4446-bb05-b93b06a2cd12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar las filas con valores nulos directamente en el DataFrame original\n",
    "df = df.dropna()\n",
    "\n",
    "# Verificar el resultado\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cbb3b1ca-24ac-4206-8906-3ecee5d6c5aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. ¿Cuántos registros tiene el csv?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "601a206f-d1f3-427f-a331-207f0e6eb7ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Contar el número de registros en el DataFrame\n",
    "num_registros = df.count()\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f\"El número total de registros es: {num_registros}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e9aac77-6fe5-4a54-b6ae-c152feee149f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Mostrar los valores únicos de `Job title` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8284db-6caf-4ff2-88a2-faee2223a00d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna \"Job Title\"\n",
    "valores_unicos = df.select(\"Job Title\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos.display()  # truncate=False para ver los valores completos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df06a700-7dd6-44c8-877c-52819ffb73a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. Remover la letra `K` de la columna `Salary Estimate` y multiplicar por 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3838764-bcb8-48ca-9d9a-0d4a7e9cadf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    (regexp_replace(col(\"Salary Estimate\"), \"K\", \"000\"))\n",
    ")\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0f2bbcd-5c78-4d10-9118-e105f611b4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Mostrar los valores únicos del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b6a270-b771-4527-b55e-f91c89d0883a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna \"Salary Estimate\"\n",
    "valores_unicos_salario = df.select(\"Salary Estimate\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos_salario.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5d10ab-fd9e-4d8b-bf7f-f8d25b9ed1af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "9. Eliminar `(Glassdoor est.)` y `(Employer est.)` del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e5085d-e01a-4a45-80ac-0523b06d8f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Eliminar \"(Glassdoor est.)\" y \"(Employer est.)\" de la columna \"Salary Estimate\"\n",
    "df = df.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    regexp_replace(col(\"Salary Estimate\"), \"\\\\(Glassdoor est.\\\\)\", \"\")  # Eliminar \"(Glassdoor est.)\"\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Salary Estimate\",\n",
    "    regexp_replace(col(\"Salary Estimate\"), \"\\\\(Employer est.\\\\)\", \"\")  # Eliminar \"(Employer est.)\"\n",
    ")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"Salary Estimate\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80ff37c4-17a0-4eb4-b170-a131bd9b7844",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "10. Mostrar de mayor a menor los valores del campo `Salary Estimate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159272e1-5acb-4c96-85e3-0a0c4c96be51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ordenar por el campo Salary Estimate de forma descendente\n",
    "df.orderBy(\"Salary Estimate\", ascending=False).select(\"Salary Estimate\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c20074ca-ff4d-45ba-a548-369bd3bdadc1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "11. De la columna `Job Description` quitar los saltos de linea `\\n` del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6ef57e-09ec-4e2c-8b45-a748c094fff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Reemplazar los saltos de línea (\\n) en la columna \"Job Description\"\n",
    "df = df.withColumn(\n",
    "    \"Job Description\",\n",
    "    regexp_replace(\"Job Description\", r\"\\n\", \" \")\n",
    ")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"Job Description\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63b8b421-3881-4bae-95c0-b79a37422e48",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "12. De la columna `Rating` muestre los valores unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e92c88c-5244-4581-b52a-8235d16dc002",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna \"Rating\"\n",
    "valores_unicos_rating = df.select(\"Rating\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos_rating.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd85fcdd-3514-4ad2-ab52-d1f21b825f5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "13. Del campo `Rating` reemplazar los `-1.0` por `0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed501b32-edad-425c-980a-3584f6ef5e60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "# Reemplazar -1.0 por 0.0 en la columna \"Rating\"\n",
    "df = df.withColumn(\n",
    "    \"Rating\",\n",
    "    when(col(\"Rating\") == -1.0, 0.0).otherwise(col(\"Rating\"))\n",
    ")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"Rating\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "640152db-ab2f-4add-893b-43ea294599b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Filtrar las filas donde Rating es 0.0\n",
    "df_zero_rating = df.filter(col(\"Rating\") == 0.0)\n",
    "\n",
    "# Mostrar los resultados\n",
    "df_zero_rating.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd30cd0d-46a3-4f2f-84f8-4c7e81d7d3ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "14. Mostrar los valores unicos y ordenar los valores del campo `Company Name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd855a8-05af-43f7-af83-1b5cd8ebd2a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna \"Company Name\" y ordenarlos alfabéticamente\n",
    "df_unique_companies = df.select(\"Company Name\").distinct().orderBy(\"Company Name\")\n",
    "\n",
    "# Mostrar los valores únicos ordenados\n",
    "df_unique_companies.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd2e2a45-3570-49a6-823c-882f5714a647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "15. Quitar todos los caracteres innecesarios que encuentres en el campo `Company Name`. Por ejemplo los saltos de linea `\\n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db5540ac-a3a4-45d3-990e-7cd71e0aba15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Eliminar saltos de línea (\\n) y otros caracteres innecesarios en \"Company Name\"\n",
    "df = df.withColumn(\n",
    "    \"Company Name\",\n",
    "    regexp_replace(\"Company Name\", r\"\\n\", \" \")  # Reemplazar saltos de línea por un espacio\n",
    ")\n",
    "\n",
    "# También puedes reemplazar otros caracteres no deseados, como espacios extra:\n",
    "df = df.withColumn(\n",
    "    \"Company Name\",\n",
    "    regexp_replace(\"Company Name\", r\"\\s+\", \" \").trim() \n",
    ")\n",
    "\n",
    "# Mostrar los resultados\n",
    "df.select(\"Company Name\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e594ec2-2d3f-429a-8cf4-7a6815567aa0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "16. En el campo `Location` convertir esa columna en dos: `City` y `State`. Las ciudades que tengas en `Location` asignar a la columna `City`. Lo mismo para `State`. Luego elimine la columna `Location`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e79265ee-28fe-4619-9a21-b71cabe76fe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Dividir la columna \"Location\" en dos columnas: \"City\" y \"State\"\n",
    "df = df.withColumn(\"City\", split(col(\"Location\"), \",\")[0]) \\\n",
    "       .withColumn(\"State\", split(col(\"Location\"), \",\")[1])\n",
    "\n",
    "# Eliminar la columna \"Location\"\n",
    "df = df.drop(\"Location\")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"City\", \"State\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ed408ad-803a-4209-9dae-57b7418724ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "17. Repetir la misma lógica de la pregunta 16 pero para el campo `Headquarters`. En Headquarters dejar solo la ciudad, mientras que para el estado añadirla a una columna nueva ` Headquarter State`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f1e2692-baad-4f0a-ae5e-458f65398829",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Dividir la columna \"Headquarters\" en dos columnas: \"City\" y \"Headquarter State\"\n",
    "df = df.withColumn(\"City\", split(col(\"Headquarters\"), \",\")[0]) \\\n",
    "       .withColumn(\"Headquarter State\", split(col(\"Headquarters\"), \",\")[1])\n",
    "\n",
    "# Eliminar la columna \"Headquarters\"\n",
    "df = df.drop(\"Headquarters\")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"City\", \"Headquarter State\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09c9f37c-dbe5-4f0e-8f9b-7c4b4cfe8ee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "18. Muestre los valores únicos del campo `Headquarter State` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce16dfdc-e525-4dea-9b78-eab4f464fcc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna \"Headquarter State\"\n",
    "valores_unicos_headquarter_state = df.select(\"Headquarter State\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos_headquarter_state.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7895f10-d9c8-443b-a225-848031030eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "19. Mostrar valores unicos del campo `Size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a341d37f-e7c3-4507-aa53-81e4aa5b1cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obtener los valores únicos de la columna \"Size\"\n",
    "valores_unicos_size = df.select(\"Size\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos_size.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8f6a6b8-52b5-4e2e-9850-6dadc97f206c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "20. Quitar 'employee' de los registros del campo `Size`. Elimine tambien otros caracteres basura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf6765b-81d4-40ac-9243-c5c8f619b114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36dfe0a0-9a25-40bc-8282-cba498d2badd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "21. Reemplazar la palabra 'to' por '-' en todos los registros del campo `Size`. Reemplazar tambien '-1' por 'Unknown'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8db42c-0a9c-4cc5-bc27-6adf4d8ccb2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Reemplazar 'to' por '-' y '-1' por 'Unknown' en la columna \"Size\"\n",
    "df = df.withColumn(\n",
    "    \"Size\",\n",
    "    regexp_replace(regexp_replace(\"Size\", \"to\", \"-\"), \"-1\", \"Unknown\")\n",
    ")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"Size\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f598578e-b86d-4de7-a298-709ec19648ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "22. Mostrar el tipo de dato del campo `Type of ownership` y sus registros unicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c633180e-4967-4a99-a2e7-b5eecf25cb65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna \"Type of ownership\"\n",
    "tipo_dato = df.select(\"Type of ownership\").dtypes[0][1]\n",
    "print(f\"Tipo de dato de 'Type of ownership': {tipo_dato}\")\n",
    "\n",
    "# Obtener los valores únicos de la columna \"Type of ownership\"\n",
    "valores_unicos_type_of_ownership = df.select(\"Type of ownership\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos_type_of_ownership.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f42ef72-f068-4b46-b1d3-6a5d59290322",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "23. Cambiar '-1' por 'Unknown' en todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02ca19eb-dd36-411f-862b-1061b2c443ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Reemplazar '-1' por 'Unknown' en la columna \"Type of ownership\"\n",
    "df = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    regexp_replace(\"Type of ownership\", \"-1\", \"Unknown\")\n",
    ")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"Type of ownership\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b9f9087-9cce-4213-b99b-f573757dbd97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "24. Cambiar:  \n",
    "-  `Company - Public` por `Public Company`  \n",
    "-  `Company - Private` por `Private Company`  \n",
    "-  `Private Practice / Firm` por `Private Company`  \n",
    "-  `Subsidiary or Business Segment` por `Business`  \n",
    "-  `College / University` por `Education`  \n",
    "En todos los registros del campo `Type of ownership`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dca3751-ad79-4cf9-86b4-af728141541b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Reemplazar los valores en la columna \"Type of ownership\"\n",
    "df = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    regexp_replace(\"Type of ownership\", \"Company - Public\", \"Public Company\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    regexp_replace(\"Type of ownership\", \"Company - Private\", \"Private Company\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    regexp_replace(\"Type of ownership\", \"Private Practice / Firm\", \"Private Company\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    regexp_replace(\"Type of ownership\", \"Subsidiary or Business Segment\", \"Business\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Type of ownership\",\n",
    "    regexp_replace(\"Type of ownership\", \"College / University\", \"Education\")\n",
    ")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"Type of ownership\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed329cf5-1daa-4dd9-9b16-d2ab23bbf486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "25. Mostrar el tipo de dato y los valores unicos del campo `Industry`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4195b438-ac58-4c0e-951e-6100422d8ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna \"Industry\"\n",
    "tipo_dato_industry = df.select(\"Industry\").dtypes[0][1]\n",
    "print(f\"Tipo de dato de 'Industry': {tipo_dato_industry}\")\n",
    "\n",
    "# Obtener los valores únicos de la columna \"Industry\"\n",
    "valores_unicos_industry = df.select(\"Industry\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos_industry.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c06a12fb-1a33-4cfa-aadb-c01e00a9c4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "26. En el mismo campo de `Industry` reemplazar '-1' por 'Not Available' y '&' por 'and'.  Vuelva a imprimir los valores unicos en orden alfabético."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fcc3594-980d-4f0b-98e0-b5adec092967",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' y '&' por 'and' en la columna \"Industry\"\n",
    "df = df.withColumn(\n",
    "    \"Industry\",\n",
    "    regexp_replace(regexp_replace(\"Industry\", \"-1\", \"Not Available\"), \"&\", \"and\")\n",
    ")\n",
    "\n",
    "# Obtener los valores únicos de la columna \"Industry\" en orden alfabético\n",
    "valores_unicos_industry = df.select(\"Industry\").distinct().orderBy(\"Industry\")\n",
    "\n",
    "# Mostrar los valores únicos en orden alfabético\n",
    "valores_unicos_industry.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2490bb6-1f88-4410-a0c4-75826c48b70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "27. Para el campo `Sector`, muestre el tipo de dato y los valores únicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17bf848a-9d8d-46a6-9ffa-fab8a99bf37b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna \"Sector\"\n",
    "tipo_dato_sector = df.select(\"Sector\").dtypes[0][1]\n",
    "print(f\"Tipo de dato de 'Sector': {tipo_dato_sector}\")\n",
    "\n",
    "# Obtener los valores únicos de la columna \"Sector\"\n",
    "valores_unicos_sector = df.select(\"Sector\").distinct()\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos_sector.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0d92b2b-c4be-4aca-b734-3ef8d80b62cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "28. Aplica la misma lógica de la pregunta 26 pero sobre el campo `Sector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc2cc3f0-9639-4d4c-b88c-f76b7bcf70ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Reemplazar '-1' por 'Not Available' y '&' por 'and' en la columna \"Sector\"\n",
    "df = df.withColumn(\n",
    "    \"Sector\",\n",
    "    regexp_replace(regexp_replace(\"Sector\", \"-1\", \"Not Available\"), \"&\", \"and\")\n",
    ")\n",
    "\n",
    "# Obtener los valores únicos de la columna \"Sector\" en orden alfabético\n",
    "valores_unicos_sector = df.select(\"Sector\").distinct().orderBy(\"Sector\")\n",
    "\n",
    "# Mostrar los valores únicos en orden alfabético\n",
    "valores_unicos_sector.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a3770cb-694e-439f-8038-d28da4e3ac0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "29. Para el campo `Revenue`, muestre el tipo de dato y los valores únicos en orden ascedente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d964167-f98b-4f7b-a787-9c38310a4783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar el tipo de dato de la columna \"Revenue\"\n",
    "tipo_dato_revenue = df.select(\"Revenue\").dtypes[0][1]\n",
    "print(f\"Tipo de dato de 'Revenue': {tipo_dato_revenue}\")\n",
    "\n",
    "# Obtener los valores únicos de la columna \"Revenue\" en orden ascendente\n",
    "valores_unicos_revenue = df.select(\"Revenue\").distinct().orderBy(\"Revenue\")\n",
    "\n",
    "# Mostrar los valores únicos en orden ascendente\n",
    "valores_unicos_revenue.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9cb56eb9-3684-49cd-b20f-fd368d23b499",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "30. En el campo `Revenue`, cambiar:  \n",
    "-  `-1` por `N/A`  \n",
    "-  `Unknown / Non-Applicable` por `N/A`  \n",
    "-  `Less than $1 million (USD)` por `Less than 1`\n",
    "-  Quitar `$` y `(USD)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b6f33f3-d318-4a34-a575-c321b7abebec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "# Reemplazar '-1' por 'N/A', 'Unknown / Non-Applicable' por 'N/A', \n",
    "# 'Less than $1 million (USD)' por 'Less than 1' y quitar '$' y '(USD)'.\n",
    "df = df.withColumn(\n",
    "    \"Revenue\",\n",
    "    regexp_replace(\"Revenue\", \"-1\", \"N/A\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Revenue\",\n",
    "    regexp_replace(\"Revenue\", \"Unknown / Non-Applicable\", \"N/A\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Revenue\",\n",
    "    regexp_replace(\"Revenue\", \"Less than \\\\$1 million \\\\(USD\\\\)\", \"Less than 1\")\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Revenue\",\n",
    "    regexp_replace(\"Revenue\", \"\\\\$\", \"\")  # Eliminar el símbolo \"$\"\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"Revenue\",\n",
    "    regexp_replace(\"Revenue\", \"\\\\(USD\\\\)\", \"\")  # Eliminar \"(USD)\"\n",
    ")\n",
    "\n",
    "# Mostrar los resultados para verificar\n",
    "df.select(\"Revenue\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d89472f2-855f-4edc-bc47-d2654069bf38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "31. Borrar el campo `Competitors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3393db34-8c8e-459c-a30d-b745678ccddb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Eliminar la columna \"Competitors\"\n",
    "df = df.drop(\"Competitors\")\n",
    "\n",
    "# Mostrar el esquema para verificar que la columna fue eliminada\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47e6892c-bf7e-4f48-9100-4722f1dee236",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d3b8e8-a020-4a19-9d85-fb3f635ed316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "32. Crear tres columnas: `min_salary` (salario mínimo), `max_salary` (salario maximo) y `avg_salary` (salario promedio) a partir de los datos del campo `Salary Estimate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6573c7f8-e8fb-4ad2-b98f-1a711f335dc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"SalarioSinDollar\", regexp_replace(col(\"Salary Estimate\"), \"\\$\", \"\"))\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"min_salary\", \n",
    "    split(col(\"SalarioSinDollar\"), \"-\")[0].cast(\"int\")  # Tomamos el primer valor como salario mínimo\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"max_salary\", \n",
    "    split(col(\"SalarioSinDollar\"), \"-\")[1].cast(\"int\")  # Tomamos el segundo valor como salario máximo\n",
    ")\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"avg_salary\", \n",
    "    (col(\"min_salary\") + col(\"max_salary\")) / 2\n",
    ")\n",
    "\n",
    "df.select(\"index\", \"Job Title\", \"min_salary\", \"max_salary\", \"avg_salary\").orderBy(col('index').asc()).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "359b377b-30ee-4c05-b7cd-f0f217a4d9e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "33. Mostrar los valores unicos del campo `Founded` y el tipo de dato."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd2dd8d-9e02-46e5-b441-0b93d2775f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "34. Reemplazar '-1' por '2024' en todos los registros del campo `Founded`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e823336d-0258-4af5-8d13-c9a358b9295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "35. Crear una nueva columna o campo que se llame `company_age` con los datos que se deducen del campo `Founded`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "49a8f7ed-434c-4ad1-ba31-eeecd65c117c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "36. Crear una columna o campo que se llame: `Job Type` y en cada registro debe ir Senior, Junior o NA según los datos del campo `Job Title`.  \n",
    "- Cambiar 'sr' o 'senior' o 'lead' o 'principal' por `Senior` en el campo `Job Type`. No olvidar las mayúsculas.\n",
    "- Cambiar 'jr' o 'jr.' o cualquier otra variante por `Junior`.  \n",
    "- En cualquier otro caso distinto a los anteriores añadir NA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c70227b-c16c-4919-9aa2-9bf9514b76dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "37. Muestra los registros únicos del campo `Job Type`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd4dbad-048d-461f-bf2a-f54c557d16f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "38. Partiendo del campo `Job Description` se extraer todas o las principales skills solicitadas por las empresas, por ejemplo: Python, Spark , Big Data. Cada Skill debe ir en una nueva columna de tipo Binaria ( 0 , 1) o Booleana (True,  False) de modo que cada skill va ser una nueva columna y si esa skill es solicitada por la empresa colocar 1 sino colocar 0. Por ejemplo:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e86af1-e86e-48db-b9d8-832cee782b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Por ejemplo:  \n",
    "| Job Title         | Salary Estimate | Job Description                                 | Rating | Company Name       | Size       | Founded | Type of ownership         | Industry                       | Sector                         | Same State      | company_age | Python | Excel |\n",
    "|--------------------|-----------------|-------------------------------------------------|--------|--------------------|------------|---------|---------------------------|--------------------------------|--------------------------------|----------------|-------------|--------|-------|\n",
    "| Sr Data Scientist | 137000-171000   | Description The Senior Data Scientist is resp... | 3.1    | Healthfirst        | 1001-5000  | 1993    | Nonprofit Organization    | Insurance Carriers            | Insurance Carriers            | Same State      | 31          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Secure our Nation, Ignite your Future Join th... | 4.2    | ManTech            | 5001-10000 | 1968    | Public Company            | Research and Development      | Research and Development      | Same State      | 56          | 0      | 0     |\n",
    "| Data Scientist    | 137000-171000   | Overview Analysis Group is one of the larges... | 3.8    | Analysis Group      | 1001-5000  | 1981    | Private Company           | Consulting                    | Consulting                    | Same State      | 43          | 1      | 1     |\n",
    "| Data Scientist    | 137000-171000   | JOB DESCRIPTION: Do you have a passion for Da... | 3.5    | INFICON            | 501-1000   | 2000    | Public Company            | Electrical and Electronic Manufacturing | Electrical and Electronic Manufacturing | Different State | 24          | 1      | 1     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5609a545-1510-4707-a0bd-a937e37675a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "39. Exportar dataset final a csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41fe24a6-5fc3-4ec4-9b76-56fd2b3e36ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "40. Extraer todos los insights posibles que sean de valor o utilidad. Cree nuevas columnas, agrupar,  filtrar hacer varios plots que muestren dichos insights que sean de utilidad para una empresa o para un usuario. Elabore conclusiones con los insights encontrados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47336ad8-9ded-4bf1-bb31-77bee1ff1d7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Caso_4",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
